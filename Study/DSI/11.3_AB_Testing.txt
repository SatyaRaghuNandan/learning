Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2016-12-14T09:15:58-08:00

====== 11.3 AB Testing ======
Created Wednesday 14 December 2016


**Qingqing Gan**
**A/B Testing**

**Why** do we do A/B testing?
A good way to **prove causality**! 
Normally we know correlation != causality, but in A/B testing, effect must have come from the changes

HiPPO: Highest Paid Person's Opinion
OEC: overall evaluation criteria
False-Positive error is "Type I" error
300 milliseconds are when human eyes can notice the lagging
impression: number of visible elements in a webpage shown on your screen
page view: an instance of an Internet user visiting a particular page on a website

To answer an interview question: 


3 metrics in A/B Testing:
successful metrics (most important)
guardrail metrics (can go up, down, not necessarily lead to bad results, depends on your current goal)

you always need to strick a balance among the metrics


**Factors that affct the false-positive rate (you decided to go with the change, but there is actually no difference between A and B):**
prevalence of real effecits (higher is good)  — prior of alternative hypothesis (domain knowldege)
Power (higher is good) — enough data points
P-value — (lower is good)
//Final Minimum Probability of True Null (False Positive Rate)//
**false-positive rate is like posterior prob of Null Hypothesis, given observed data, it may be large even when P-value < 5% if the prior prob of Null Hypo is high (not all P-values are equally helpful)**

http://blog.minitab.com/blog/adventures-in-statistics-2/understanding-hypothesis-tests:-confidence-intervals-and-confidence-levels
http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-correctly-interpret-p-values
http://blog.minitab.com/blog/adventures-in-statistics-2/not-all-p-values-are-created-equal


**SRM (Sample Ratio Mismatch)**
Quara question: 
Welch's T-test: statistical tricks you can use when you have unequal sized samples... (discouraged, you need to be very sharp to use it right!)
P29 of 39: main reason you don't want a bad experiment (e.g. SRM), there are still bugs/problems even if you can compensate with statistical tricks!


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
**Stephen Hsiao:**
**Metrics** 

If the result of you A/B testing is bad, don't just give up, go back and deep dive!!!

Typical Metrics:
customer engagement
monetization
How to adjust the business strategy? Don't Vote! Do A/B Testing!!! 

Three types of metrics:
1. success metrics (your objective goal)
2. guardrail metrics (it tells you lots about your customers, but sometimes too sensitive)
3. Future metrics

Be aware of the concept of Trigger (e.g. ordering dissert in a restaurant)

